{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image recognition with TTR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bridging between perceptual and conceptual domains\n",
    "\n",
    "Let's apply the object detection representation proposed in Dobnik & Cooper's *Interfacing language, spatial perception and cognition in TTR* to image recognition.\n",
    "\n",
    "![Fig 8](fig/lspc-fig8.png)\n",
    "\n",
    "Here, we use `Image` instead of `PointMap` for the whole, but instead of `reg:PointMap` we use yet another type (and rename it), `seg:Segment`. In Cooper's case the same type can be used to represent both the region and the whole, because a `PointMap` is a set of absolute positions. With `Image`, positions are relative to an origin, which needs to be specified when cropping.\n",
    "\n",
    "I guess in the general case, the domain of an `ObjectDetector` function need not be the same as the `reg` fields in the output elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1080x1080 at 0x7F5C749A7908>'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('pyttr')\n",
    "from pyttr.ttrtypes import *\n",
    "from pyttr.utils import *\n",
    "import PIL.Image\n",
    "\n",
    "ttrace()\n",
    "\n",
    "# Basic types.\n",
    "\n",
    "Ind = BType('Ind')\n",
    "\n",
    "Int = BType('Int')\n",
    "Int.learn_witness_condition(lambda x: isinstance(x, int))\n",
    "print(Int.query(365))\n",
    "\n",
    "Image = BType('Image')\n",
    "Image.learn_witness_condition(lambda x: isinstance(x, PIL.Image.Image))\n",
    "img = PIL.Image.open('res/dogcar.jpg')\n",
    "print(Image.query(img))\n",
    "\n",
    "# Segment type: a rectangular area of a given image.\n",
    "\n",
    "Segment = RecType({'i': Image, 'cx': Int, 'cy': Int, 'w': Int, 'h': Int})\n",
    "print(Segment.query(Rec({'i': img, 'cx': 100, 'cy': 150, 'w': 40, 'h': 20})))\n",
    "\n",
    "# Redefine Image.show() to work with Rec.show().\n",
    "def image_show(self):\n",
    "    return str(self)\n",
    "PIL.Image.Image.show = image_show\n",
    "show(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ppty = FunType(Ind, Ty)\n",
    "ImageDetection = RecType({'seg': Segment, 'pfun': Ppty})\n",
    "ImageDetections = ListType(ImageDetection)\n",
    "ObjectDetector = FunType(Image, ImageDetections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection model YOLO\n",
    "\n",
    "Requires OpenCV and [Darkflow](https://github.com/thtrieu/darkflow). `yolo.weights` is from [Yolo](https://pjreddie.com/darknet/yolo/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing yolo/yolo.cfg\n",
      "Loading yolo/yolo.weights ...\n",
      "Successfully identified 203934260 bytes\n",
      "Finished in 0.022200584411621094s\n",
      "Model has a coco model name, loading coco labels.\n",
      "\n",
      "Building net ...\n",
      "Source | Train? | Layer description                | Output size\n",
      "-------+--------+----------------------------------+---------------\n",
      "       |        | input                            | (?, 608, 608, 3)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 608, 608, 32)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 304, 304, 32)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 304, 304, 64)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 152, 152, 64)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 152, 152, 128)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 152, 152, 64)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 152, 152, 128)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 76, 76, 128)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 76, 76, 256)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 76, 76, 128)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 76, 76, 256)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 38, 38, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 19, 19, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 19, 19, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 19, 19, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | concat [16]                      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 64)\n",
      " Load  |  Yep!  | local flatten 2x2                | (?, 19, 19, 256)\n",
      " Load  |  Yep!  | concat [27, 24]                  | (?, 19, 19, 1280)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 1x1p0_1    linear           | (?, 19, 19, 425)\n",
      "-------+--------+----------------------------------+---------------\n",
      "Running entirely on CPU\n",
      "Finished in 9.431905031204224s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from darkflow.net.build import TFNet\n",
    "\n",
    "tfnet = TFNet({\"model\": \"yolo/yolo.cfg\", \"load\": \"yolo/yolo.weights\",\n",
    "    'config': 'yolo', \"threshold\": 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cx': 20, 'cy': 30, 'w': 20, 'h': 20}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def xy1xy2_to_cwh(x1, y1, x2, y2):\n",
    "    '''Transform (x1, y1, x2, y2) to (x_center, y_center, width, height).'''\n",
    "    return {'cx': int(x1/2 + x2/2), 'cy': int(y1/2 + y2/2), 'w': x2 - x1, 'h': y2 - y1}\n",
    "print(xy1xy2_to_cwh(10, 20, 30, 40))\n",
    "\n",
    "def yolo_detector(i):\n",
    "    return [Rec({\n",
    "        'seg': Rec({\n",
    "            'i': i,\n",
    "            **xy1xy2_to_cwh(o['topleft']['x'], o['topleft']['y'], o['bottomright']['x'], o['bottomright']['y']),\n",
    "        }),\n",
    "        'pfun': Fun('v', Ind, PType(Pred(o['label'], [Ind]), ['v'])),\n",
    "    }) for o in tfnet.return_predict(np.array(i))] # @todo RBG/BGR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "[{seg = {cy = 654, i = <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1080x1080 at 0x7F5C749A7908>, w = 276, cx = 138, h = 809}, pfun = lambda v:Ind . person(v)}, {seg = {cx = 714, i = <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1080x1080 at 0x7F5C749A7908>, cy = 657, w = 706, h = 796}, pfun = lambda v:Ind . person(v)}, {seg = {cx = 194, i = <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1080x1080 at 0x7F5C749A7908>, cy = 888, w = 380, h = 381}, pfun = lambda v:Ind . person(v)}, {seg = {cx = 490, i = <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1080x1080 at 0x7F5C749A7908>, cy = 589, w = 774, h = 979}, pfun = lambda v:Ind . car(v)}, {seg = {cx = 704, i = <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1080x1080 at 0x7F5C749A7908>, cy = 714, w = 687, h = 718}, pfun = lambda v:Ind . dog(v)}, {seg = {cx = 757, i = <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1080x1080 at 0x7F5C749A7908>, cy = 541, w = 219, h = 210}, pfun = lambda v:Ind . chair(v)}, {seg = {cx = 547, i = <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1080x1080 at 0x7F5C749A7908>, cy = 687, w = 778, h = 783}, pfun = lambda v:Ind . chair(v)}, {seg = {cx = 486, i = <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1080x1080 at 0x7F5C749A7908>, cy = 677, w = 957, h = 803}, pfun = lambda v:Ind . sofa(v)}, {seg = {cx = 93, i = <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1080x1080 at 0x7F5C749A7908>, cy = 588, w = 187, h = 423}, pfun = lambda v:Ind . cell phone(v)}, {seg = {cx = 44, i = <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1080x1080 at 0x7F5C749A7908>, cy = 544, w = 71, h = 107}, pfun = lambda v:Ind . clock(v)}]\n"
     ]
    }
   ],
   "source": [
    "image_detections = yolo_detector(img)\n",
    "\n",
    "print(ImageDetections.query(image_detections))\n",
    "print(ImageDetection.query(image_detections[0]))\n",
    "print(Ppty.query(image_detections[0].pathvalue('pfun')))\n",
    "print(Segment.query(image_detections[0].pathvalue('seg')))\n",
    "\n",
    "print(show(image_detections))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individuation function\n",
    "\n",
    "According to lspc.ipynb, this cannot be implemented entirely within PyTTR, we need a Python function to do it (with PyTTR input & output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{c : (lambda v:Ind . person(v), [a]), loc : (lambda v:Ind . location(v, {cy = 654, i = <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1080x1080 at 0x7F5C749A7908>, w = 276, cx = 138, h = 809}), [a]), a : Ind}\n"
     ]
    }
   ],
   "source": [
    "pred_location = Pred('location', [Ind, ImageDetection])\n",
    "def ind_fun(r):\n",
    "    if not ImageDetection.query(r):\n",
    "        return None\n",
    "    return RecType({\n",
    "        'a': Ind,\n",
    "        'loc': (Fun('v', Ind, PType(pred_location, ['v', r.seg])), ['a']),\n",
    "        'c': (r.pfun, ['a']),\n",
    "        # Why is property (function?) application represented with\n",
    "        # just a <fun, arg> tuple? How does that help us?\n",
    "    })\n",
    "\n",
    "print(show(ind_fun(image_detections[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sketching on spatial relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left(a, b)\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "pred_left = Pred('left', [Segment, Segment])\n",
    "Left = PType(pred_left, ['a', 'b'])\n",
    "Left.learn_witness_condition(lambda ab: ab[0].cx < ab[1].cx)\n",
    "print(show(Left))\n",
    "print(Left.validate())\n",
    "\n",
    "print(Left.query((image_detections[0].seg, image_detections[1].seg)))\n",
    "print(Left.query((image_detections[2].seg, image_detections[2].seg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
